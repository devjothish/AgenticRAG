{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dDjlI_KTZP0G",
        "outputId": "57726148-6fb4-4fd0-cc01-8901e5c42603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet athina-client langchain langchain_community pypdf langchain-deepseek-official langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client langchain langchain-google-genai google-generativeai\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EC0bvXachaj3",
        "outputId": "1456d190-00db-45c6-a9c8-080449f19dc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.13.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.26.4)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.3.0)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.12.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Downloading qdrant_client-1.13.3-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, portalocker, grpcio-tools, qdrant-client\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "Successfully installed grpcio-tools-1.71.0 portalocker-2.10.1 protobuf-5.29.3 qdrant-client-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set api key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['ATHINA_API_KEY'] = '#enteryours'\n",
        "\n",
        "os.environ['TAVILY_API_KEY'] = '#enteryours'\n",
        "os.environ['DEEPSEEK_API_KEY'] = '#enteryours'\n",
        "os.environ['QDRANT_API_KEY'] = '#enteryours'\n"
      ],
      "metadata": {
        "id": "ix_bA7u5azxY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pdf\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/test.pdf\")\n",
        "documents = loader.load()\n",
        ""
      ],
      "metadata": {
        "id": "FzaEBih3c2Ti"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "XGzBo6UMdOUe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load embedding model\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", encode_kwargs = {\"normalize_embeddings\": True})"
      ],
      "metadata": {
        "id": "btSupjdddO5M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Qdrant Vector Store**"
      ],
      "metadata": {
        "id": "o0riQRalddSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create vectorstore\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    url=\"#yourclusterinstance\",\n",
        "    prefer_grpc=True,\n",
        "    collection_name=\"documents\",\n",
        "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "bRMzIXJfdTJp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create retriever\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "5uw4hEf5ivFz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WEB SEARCH**"
      ],
      "metadata": {
        "id": "memDd5Mki1Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define web search\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(k=10)"
      ],
      "metadata": {
        "id": "waZqlhgLixB6"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.run(\"Tesla stock market summary for Q3?\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3h_xjvCi7ul",
        "outputId": "2d4b1e93-d418-40d2-99dd-a866d599cc96"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': \"What Drove Tesla's Q3 Earnings Beat? - Nasdaq\",\n",
              "  'url': 'https://www.nasdaq.com/articles/what-drove-teslas-q3-earnings-beat',\n",
              "  'content': 'Tesla stock (NASDAQ:TSLA) posted a better-than-expected set of Q3 2024 earnings. Revenue for the quarter rose 8% year-over-year to $25.18 billion a year',\n",
              "  'score': 0.86303437},\n",
              " {'title': 'Tesla Earnings Unexpectedly Rise 9% In Q3, Elon Musk Bullish',\n",
              "  'url': 'https://www.investors.com/news/tesla-stock-earnings-q3-elon-musk/',\n",
              "  'content': 'The electric-vehicle giant reported Wednesday that Q3 earnings grew 9% to 72 cents per share. Meanwhile, revenue totaled $25.18 billion, up 8%',\n",
              "  'score': 0.8482825},\n",
              " {'title': 'Tesla (TSLA) Q3 2024 earnings report - CNBC',\n",
              "  'url': 'https://www.cnbc.com/2024/10/23/tesla-tsla-q3-2024-earnings-report.html',\n",
              "  'content': 'Best Credit Cards Best Travel Credit Cards Best Loans to Refinance Credit Card Debt Best Credit Monitoring Services Best Credit Cards for Small Business Best Personal Loans for Bad Credit Best Mortgages for Bad Credit CEO Elon Musk said on the earnings call that his \"best guess\" is that \"vehicle growth\" will reach 20% to 30% next year, due to \"lower cost vehicles\" and the \"advent of autonomy.\" Analysts surveyed by FactSet were expecting a total increase in deliveries next year of about 15% to 2.04 million. In a shareholder deck, Tesla said the Cybertruck, became the third-best-selling fully electric vehicle in the U.S., behind only the Model 3 and Model Y.',\n",
              "  'score': 0.69746524},\n",
              " {'title': 'Tesla, Inc. (TSLA) Analyst Ratings, Estimates & Forecasts',\n",
              "  'url': 'https://finance.yahoo.com/quote/TSLA/analysis/',\n",
              "  'content': \"(TSLA) Analyst Ratings, Estimates & Forecasts - Yahoo Finance Health news Stock Market Futures Fantasy Yahoo Finance Yahoo Finance Fantasy Stock Market Futures Estimate Trends Earnings Estimate | Currency in USD | Current Qtr. | Currency in USD | Current Qtr. | Currency in USD | Current Qtr. | Current Estimate | 0.52 | 0.68 | 2.84 | 3.77 | | Currency in USD | Current Qtr. Yahoo Finance’s Top Analysts section provides an objective scorecard to evaluate the accuracy of Wall Street analyst ratings and price targets. Mortgages Credit Cards Sectors Crypto Heatmap Financial News Data Disclaimer Help Feedback Sitemap Licensing What's New About Our Ads Premium Plans S&P Futures 5,994.25 -6.50 (-0.11%) Ad Terms Feedback More Info\",\n",
              "  'score': 0.42763633},\n",
              " {'title': 'Tesla, Inc. (TSLA) Stock Price, News, Quote & History - Yahoo Finance',\n",
              "  'url': 'https://finance.yahoo.com/quote/TSLA/',\n",
              "  'content': '(TSLA) Stock Price, News, Quote & History - Yahoo Finance This segment also provides sedans and sport utility vehicles through direct and used vehicle sales, a network of Tesla Superchargers, and in-app upgrades; purchase financing and leasing services; services for electric vehicles through its company-owned service locations and Tesla mobile service technicians; and vehicle limited warranties and extended service plans. The Energy Generation and Storage segment engages in the design, manufacture, installation, sale, and leasing of solar energy generation and energy storage products, and related services to residential, commercial, and industrial customers and utilities through its website, stores, and galleries, as well as through a network of channel partners; and provision of service and repairs to its energy product customers, including under warranty, as well as various financing options to its solar customers.',\n",
              "  'score': 0.21668245}]"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Agentic RAG*"
      ],
      "metadata": {
        "id": "14Rv7GxBjCuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7GHK_nQotge",
        "outputId": "25a2464b-0b7f-40d0-bdf1-15aaee643a5e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GEMINI_API\"] = \"#enteryours\"\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API\"])\n"
      ],
      "metadata": {
        "id": "SIyXEpS6oxcR"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=os.environ[\"GEMINI_API\"]\n",
        ")"
      ],
      "metadata": {
        "id": "sOsMnQXxi8pm"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vector search\n",
        "from langchain.chains import RetrievalQA\n",
        "def vector_search(query: str):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "    return qa_chain.run(query)"
      ],
      "metadata": {
        "id": "TjDEV2dHjFIJ"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define web search\n",
        "def web_search(query: str):\n",
        "   return web_search_tool.run(query)"
      ],
      "metadata": {
        "id": "hPilCAKljVK8"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tool call for vector search and web search\n",
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def vector_search_tool(query: str) -> str:\n",
        "    return vector_search(query)\n",
        "\n",
        "@tool\n",
        "def web_search_tool_func(query: str) -> str:\n",
        "    return web_search(query)"
      ],
      "metadata": {
        "id": "jIt8c8_tjWvp"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tools for the agent\n",
        "from langchain.agents import Tool\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"VectorStoreSearch\",\n",
        "        func=vector_search_tool,\n",
        "        description=\"Use this to search the vector store for information.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"WebSearch\",\n",
        "        func=web_search_tool_func,\n",
        "        description=\"Use this to perform a web search for information.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "5q_3fW8RjY51"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define agent prompt\n",
        "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\n",
        "Always try the \\\"VectorStoreSearch\\\" tool first. Only use \\\"WebSearch\\\" if the vector store does not contain the required information.\n",
        "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
        "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
        "Provide only ONE action per\n",
        "JSON_BLOB\n",
        "```\n",
        "Observation: action result\n",
        "... (repeat Thought/Action/Observation N times)\n",
        "Thought: I know what to respond\n",
        "Action:\n",
        "```\n",
        "{{\n",
        "  \"action\": \"Final Answer\",\n",
        "  \"action_input\": \"Final response to human\"\n",
        "}}\n",
        "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
        "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\""
      ],
      "metadata": {
        "id": "zgwFUDzKjeHF"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# human prompt\n",
        "human_prompt = \"\"\"{input}\n",
        "{agent_scratchpad}\n",
        "(reminder to always respond in a JSON blob)\"\"\"\n"
      ],
      "metadata": {
        "id": "vfwWUQA8jg3l"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt template\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", human_prompt),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "v9PgN3LAjhVM"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tool render\n",
        "from langchain.tools.render import render_text_description_and_args\n",
        "prompt = prompt.partial(\n",
        "    tools=render_text_description_and_args(list(tools)),\n",
        "    tool_names=\", \".join([t.name for t in tools]),\n",
        ")"
      ],
      "metadata": {
        "id": "IJmiyvshjogR"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create rag chain\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | JSONAgentOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "fPgrXV0bjqH8"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create agent\n",
        "from langchain.agents import AgentExecutor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=chain,\n",
        "    tools=tools,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=True\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "F1wjhcFWjsBA"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the difference between Agentic RAG vs naive RAG?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06SYCsKjtsc",
        "outputId": "bce57a8f-3f87-408d-b9d6-bf75dd5926c1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:```\n",
            "{\n",
            " \"action\": \"VectorStoreSearch\",\n",
            " \"action_input\": \"What is the difference between Agentic RAG vs naive RAG?\"\n",
            "}```\u001b[0m\u001b[36;1m\u001b[1;3mAgentic RAG uses autonomous agents to manage retrieval, filter information, and refine responses, allowing for dynamic and adaptable information processing.  Traditional RAG systems have static workflows and limited adaptability, struggling with dynamic, multi-step reasoning and complex tasks.\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Agentic RAG uses autonomous agents to manage retrieval, filter information, and refine responses, making it more dynamic and adaptable.  In contrast, traditional RAG systems follow static workflows and struggle with complex reasoning and multi-step tasks due to their limited adaptability.\"\n",
            "}```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the difference between Agentic RAG vs naive RAG?',\n",
              " 'output': 'Agentic RAG uses autonomous agents to manage retrieval, filter information, and refine responses, making it more dynamic and adaptable.  In contrast, traditional RAG systems follow static workflows and struggle with complex reasoning and multi-step tasks due to their limited adaptability.'}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"Difference between Gemini and Deepseek?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_dTy-HLoSHA",
        "outputId": "2d58ea2c-50d8-4758-bb86-e03644f48bba"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:```\n",
            "{\n",
            " \"action\": \"VectorStoreSearch\",\n",
            " \"action_input\": \"Difference between Gemini and Deepseek\"\n",
            "}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mI am sorry, but I cannot provide a comparison between Gemini and Deepseek. I do not have the context to answer your question.\u001b[0m\u001b[32;1m\u001b[1;3mAction:```{\n",
            " \"action\": \"WebSearch\",\n",
            " \"action_input\": \"Difference between Gemini and Deepseek\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3m[{'title': 'ChatGPT vs. Gemini vs. DeepSeek Understanding the Key Differences', 'url': 'https://duke-lavlesh.medium.com/chatgpt-vs-gemini-vs-deepseek-understanding-the-key-differences-b8a0c2ece25a', 'content': 'Gemini: Highly customizable for multimodal applications but may require specialized expertise. DeepSeek: Designed for easy integration into', 'score': 0.92392945}, {'title': \"I tried out DeepSeek, but I'm sticking with Gemini for now\", 'url': 'https://www.androidauthority.com/deepseek-vs-gemini-3521178/', 'content': 'Gemini broke things down into high-end, mid-range, and foldable models, while DeepSeek suggested target users for each model. I prefer DeepSeek', 'score': 0.88372874}, {'title': 'DeepSeek vs. Gemini Deep Research: Which Model Is King?', 'url': 'https://tremendous.blog/2025/01/28/deepseek-vs-gemini-deep-research-which-model-is-king/', 'content': \"Gemini does it for $20/month. It also works dramatically faster than any human, 24 hours a day. DeepSeek isn't bad for a new model.\", 'score': 0.7900289}, {'title': \"I tested DeepSeek vs Gemini AI with 7 prompts — here's the winner\", 'url': 'https://www.tomsguide.com/ai/i-tested-deepseek-vs-gemini-ai-with-7-prompts-heres-the-winner', 'content': \"I tested DeepSeek and Gemini AI with 7 prompts — here's the surprising winner | Tom's Guide I tested DeepSeek vs Gemini AI with 7 prompts — here's the winner I have a Gemini Advanced subscription, but to keep this comparison fair, I decided to use the prompts on Google Gemini’s free tier since DeepSeek is also free. Winner: Gemini AI wins for actually searching the web when I wanted it to and coming up with accurate answers in real-time. After conducting a series of tests evaluating creativity, reasoning, real-time web search capabilities, and ethical analysis, Google Gemini emerged as the superior AI. 5 DeepSeek prompts to try first — see what this new AI model can do I tested ChatGPT Plus vs Gemini Advanced with 7 prompts — here’s the winner\", 'score': 0.75464433}, {'title': 'DeepSeek R1 with search vs Perplexity pro vs gemini deep ... - Reddit', 'url': 'https://www.reddit.com/r/perplexity_ai/comments/1i9016v/deepseek_r1_with_search_vs_perplexity_pro_vs/', 'content': \"Since I started using it 8 months ago or so I've loved Perplexity. I haven't given DeepSeek enough of a chance to compare it, but Gemini is\", 'score': 0.6437107}]\u001b[0m\u001b[32;1m\u001b[1;3mAction:```\n",
            "{\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"DeepSeek and Gemini are both AI models.  Gemini is developed by Google and offers both free and paid versions (Gemini Advanced).  DeepSeek is also free to use.  Reviews comparing the two models often highlight Gemini's stronger real-time web search capabilities and overall accuracy.  While DeepSeek is a newer model and shows promise, Gemini is frequently chosen as the preferred option in direct comparisons.\"\n",
            "}```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Difference between Gemini and Deepseek?',\n",
              " 'output': \"DeepSeek and Gemini are both AI models.  Gemini is developed by Google and offers both free and paid versions (Gemini Advanced).  DeepSeek is also free to use.  Reviews comparing the two models often highlight Gemini's stronger real-time web search capabilities and overall accuracy.  While DeepSeek is a newer model and shows promise, Gemini is frequently chosen as the preferred option in direct comparisons.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Query Data"
      ],
      "metadata": {
        "id": "Us9gKKSVqmll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create agent with verbose=False for production\n",
        "agent_output = AgentExecutor(\n",
        "    agent=chain,\n",
        "    tools=tools,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "K7lZST_fqMkg"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "question = [\n",
        "    \"Difference between Gemini and Deepseek?\",\n",
        "    \"What is the difference between Agentic RAG vs naive RAG?\"\n",
        "]\n",
        "response = []\n",
        "contexts = []\n",
        "\n",
        "# Inference\n",
        "for query in question:\n",
        "    vector_contexts = retriever.get_relevant_documents(query)\n",
        "    if vector_contexts:\n",
        "        context_texts = [doc.page_content for doc in vector_contexts]\n",
        "        contexts.append(context_texts)\n",
        "    else:\n",
        "        print(f\"[DEBUG] No relevant information in vector store for query: {query}. Falling back to web search.\")\n",
        "        web_results = web_search_tool.run(query)\n",
        "        contexts.append([web_results])\n",
        "\n",
        "    # Get the agent response\n",
        "    result = agent_output.invoke({\"input\": query})\n",
        "    response.append(result['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FtwdSDjqrag",
        "outputId": "886994db-9430-4c51-e9c3-b5a58d55edad"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-754372c8a142>:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  vector_contexts = retriever.get_relevant_documents(query)\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# To dict\n",
        "data = {\n",
        "    \"query\": question,\n",
        "    \"response\": response,\n",
        "    \"context\": contexts,\n",
        "}"
      ],
      "metadata": {
        "id": "VsOIqnzKqt7m"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to Athina IDE"
      ],
      "metadata": {
        "id": "P6shfnBwq0c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the data for Athina\n",
        "rows = []\n",
        "for i in range(len(data[\"query\"])):\n",
        "    row = {\n",
        "        'query': data[\"query\"][i],\n",
        "        'context': data[\"context\"][i],\n",
        "        'response': data[\"response\"][i],\n",
        "    }\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "aq04nqtKqyUa"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to Athina\n",
        "from athina_client.datasets import Dataset\n",
        "from athina_client.keys import AthinaApiKey\n",
        "\n",
        "AthinaApiKey.set_key(os.environ['ATHINA_API_KEY'])\n",
        "\n",
        "try:\n",
        "    Dataset.add_rows(\n",
        "        dataset_id='7fa5efad-4ac5-40d1-960c-40183dc52a29',\n",
        "        rows=rows\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to add rows: {e}\")"
      ],
      "metadata": {
        "id": "jD-IZs48q19o"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}